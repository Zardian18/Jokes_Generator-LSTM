{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjIlGA9ZbbeXXS3QoPQwVb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zardian18/Jokes_Generator-LSTM/blob/master/LSTM_jokes_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWcm_RmNsEDy",
        "outputId": "c648f631-e1c8-4535-a90d-f06047c69432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-11 23:03:24--  https://raw.githubusercontent.com/Zardian18/helper-functions-colab/master/helper.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17171 (17K) [text/plain]\n",
            "Saving to: ‘helper.py’\n",
            "\n",
            "\rhelper.py             0%[                    ]       0  --.-KB/s               \rhelper.py           100%[===================>]  16.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-11 23:03:24 (37.0 MB/s) - ‘helper.py’ saved [17171/17171]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/Zardian18/helper-functions-colab/master/helper.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmTPu_lnsJI0",
        "outputId": "2d794db0-ae20-40e4-ba10-d92056b7e324"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_zUWbJxsKb-",
        "outputId": "14cb213b-419a-4092-d469-00e7a809ded1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "KVdA6iI1sL6F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "gP5DTXpNsNSh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "I85rdyqysOym"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d thedevastator/one-million-reddit-jokes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgmlNHuGsQis",
        "outputId": "43f9924a-705f-4881-cf5c-197856e9c84c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading one-million-reddit-jokes.zip to /content\n",
            " 79% 73.0M/93.0M [00:01<00:00, 62.6MB/s]\n",
            "100% 93.0M/93.0M [00:01<00:00, 81.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip one-million-reddit-jokes.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkN85It3sWGz",
        "outputId": "ad89e664-b288-44b7-d349-08a672c3bdcf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  one-million-reddit-jokes.zip\n",
            "  inflating: one-million-reddit-jokes.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RxCz-QUMsaXs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jokes_df = pd.read_csv(\"one-million-reddit-jokes.csv\")\n",
        "jokes_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "KRfLR3G9sfoY",
        "outputId": "58d4191e-ed02-4d29-d178-9ae630f20881"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   type      id subreddit.id subreddit.name  subreddit.nsfw  created_utc  \\\n",
              "0  post  ftbp1i        2qh72          jokes           False   1585785543   \n",
              "1  post  ftboup        2qh72          jokes           False   1585785522   \n",
              "2  post  ftbopj        2qh72          jokes           False   1585785508   \n",
              "3  post  ftbnxh        2qh72          jokes           False   1585785428   \n",
              "4  post  ftbjpg        2qh72          jokes           False   1585785009   \n",
              "\n",
              "                                           permalink      domain  url  \\\n",
              "0  https://old.reddit.com/r/Jokes/comments/ftbp1i...  self.jokes  NaN   \n",
              "1  https://old.reddit.com/r/Jokes/comments/ftboup...  self.jokes  NaN   \n",
              "2  https://old.reddit.com/r/Jokes/comments/ftbopj...  self.jokes  NaN   \n",
              "3  https://old.reddit.com/r/Jokes/comments/ftbnxh...  self.jokes  NaN   \n",
              "4  https://old.reddit.com/r/Jokes/comments/ftbjpg...  self.jokes  NaN   \n",
              "\n",
              "                                            selftext  \\\n",
              "0  My corona is covered with foreskin so it is no...   \n",
              "1                         It's called Google Sheets.   \n",
              "2  The vacuum doesn't snore after sex.\\r\\n\\r\\n&am...   \n",
              "3                                          [removed]   \n",
              "4                                          [removed]   \n",
              "\n",
              "                                               title  score  \n",
              "0               I am soooo glad I'm not circumcised!      2  \n",
              "1  Did you know Google now has a platform for rec...      9  \n",
              "2  What is the difference between my wife and my ...     15  \n",
              "3                              My last joke for now.      9  \n",
              "4              The Nintendo 64 turns 18 this week...    134  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef76f4b5-ff37-45a5-a8b7-84e116f7270d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>id</th>\n",
              "      <th>subreddit.id</th>\n",
              "      <th>subreddit.name</th>\n",
              "      <th>subreddit.nsfw</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>permalink</th>\n",
              "      <th>domain</th>\n",
              "      <th>url</th>\n",
              "      <th>selftext</th>\n",
              "      <th>title</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>post</td>\n",
              "      <td>ftbp1i</td>\n",
              "      <td>2qh72</td>\n",
              "      <td>jokes</td>\n",
              "      <td>False</td>\n",
              "      <td>1585785543</td>\n",
              "      <td>https://old.reddit.com/r/Jokes/comments/ftbp1i...</td>\n",
              "      <td>self.jokes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>My corona is covered with foreskin so it is no...</td>\n",
              "      <td>I am soooo glad I'm not circumcised!</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>post</td>\n",
              "      <td>ftboup</td>\n",
              "      <td>2qh72</td>\n",
              "      <td>jokes</td>\n",
              "      <td>False</td>\n",
              "      <td>1585785522</td>\n",
              "      <td>https://old.reddit.com/r/Jokes/comments/ftboup...</td>\n",
              "      <td>self.jokes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It's called Google Sheets.</td>\n",
              "      <td>Did you know Google now has a platform for rec...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>post</td>\n",
              "      <td>ftbopj</td>\n",
              "      <td>2qh72</td>\n",
              "      <td>jokes</td>\n",
              "      <td>False</td>\n",
              "      <td>1585785508</td>\n",
              "      <td>https://old.reddit.com/r/Jokes/comments/ftbopj...</td>\n",
              "      <td>self.jokes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The vacuum doesn't snore after sex.\\r\\n\\r\\n&amp;am...</td>\n",
              "      <td>What is the difference between my wife and my ...</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>post</td>\n",
              "      <td>ftbnxh</td>\n",
              "      <td>2qh72</td>\n",
              "      <td>jokes</td>\n",
              "      <td>False</td>\n",
              "      <td>1585785428</td>\n",
              "      <td>https://old.reddit.com/r/Jokes/comments/ftbnxh...</td>\n",
              "      <td>self.jokes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[removed]</td>\n",
              "      <td>My last joke for now.</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>post</td>\n",
              "      <td>ftbjpg</td>\n",
              "      <td>2qh72</td>\n",
              "      <td>jokes</td>\n",
              "      <td>False</td>\n",
              "      <td>1585785009</td>\n",
              "      <td>https://old.reddit.com/r/Jokes/comments/ftbjpg...</td>\n",
              "      <td>self.jokes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[removed]</td>\n",
              "      <td>The Nintendo 64 turns 18 this week...</td>\n",
              "      <td>134</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef76f4b5-ff37-45a5-a8b7-84e116f7270d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ef76f4b5-ff37-45a5-a8b7-84e116f7270d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ef76f4b5-ff37-45a5-a8b7-84e116f7270d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-72f94fb6-2e08-4e67-b252-a49d38bc16d8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72f94fb6-2e08-4e67-b252-a49d38bc16d8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-72f94fb6-2e08-4e67-b252-a49d38bc16d8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jokes_list=[]\n",
        "\n",
        "jokes_list.append(jokes_df[\"title\"])\n",
        "jokes_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2zjqXZ0sn-V",
        "outputId": "b527a4a8-7847-44ac-ffc8-fbd0f71c2ab3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0                      I am soooo glad I'm not circumcised!\n",
              " 1         Did you know Google now has a platform for rec...\n",
              " 2         What is the difference between my wife and my ...\n",
              " 3                                     My last joke for now.\n",
              " 4                     The Nintendo 64 turns 18 this week...\n",
              "                                 ...                        \n",
              " 999993                         With Zyan Malik leaving 1D..\n",
              " 999994                Why did the exterminator go to Italy?\n",
              " 999995    What did Arnold Schwarzenegger say when invite...\n",
              " 999996                                        The Moth Joke\n",
              " 999997                    Why did the pig go to the fridge?\n",
              " Name: title, Length: 999998, dtype: object]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(jokes_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVp1cufNs9rp",
        "outputId": "e9b1860d-314d-483d-9a19-f45d37bf1778"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999998"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jokes_list_final=[]\n",
        "\n",
        "for i in range(len(jokes_list[0])):\n",
        "  jokes_list_final.append(jokes_list[0][i])\n",
        "\n",
        "jokes_list_final[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGz82EhBtOcU",
        "outputId": "c94ebef5-3a08-4f44-a20f-03611612885f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I am soooo glad I'm not circumcised!\",\n",
              " 'Did you know Google now has a platform for recording your bowel movements?',\n",
              " 'What is the difference between my wife and my vacuum?',\n",
              " 'My last joke for now.',\n",
              " 'The Nintendo 64 turns 18 this week...']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jokes_list_final[25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-jKzeWYJte8P",
        "outputId": "baa18bbc-14cd-4f02-8d9f-8a1d8d4bc172"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tee Pee a House on, April Fool's, 2020\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_punctuation(s):\n",
        "  s = re.sub(f\"([{string.punctuation}])\", r' \\1', s)\n",
        "  s = re.sub(\" +\", \" \", s)\n",
        "  return s"
      ],
      "metadata": {
        "id": "vSqaLP2NtiJu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [pad_punctuation(x) for x in jokes_list_final]"
      ],
      "metadata": {
        "id": "rOs4_ltdtvl-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data[25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8sztjNCDt0iA",
        "outputId": "df2a14cd-9ec3-497c-e0cb-72a9e68562d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tee Pee a House on , April Fool 's , 2020\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ds = tf.data.Dataset.from_tensor_slices(text_data).batch(128).shuffle(1000)"
      ],
      "metadata": {
        "id": "CvlyA5Zpt4f1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r7U-75wuCKb",
        "outputId": "da6880a1-10db-4f13-d395-3e1cc4473386"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_ShuffleDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum =0\n",
        "for i in range(len(text_data)):\n",
        "  words = text_data[i].split()\n",
        "  sum += len(words)\n",
        "\n",
        "print(f\"Average words per joke: {sum/len(text_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UUoiFbBuDlr",
        "outputId": "8c5d6519-27e5-43bb-f39b-c0d63d1aa1a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average words per joke: 10.840525681051362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize= \"lower\",\n",
        "    max_tokens = 15000,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = 16+1,\n",
        ")"
      ],
      "metadata": {
        "id": "OgCehlM7uIJn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "metadata": {
        "id": "GhUcxJPjuNgc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVT4oaq8uPQy",
        "outputId": "1eb03eab-ca54-43b3-bec5-8e60f214a6b1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '.', 'a', 'the', '?', 'i', 'to', 'what', 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaLmBrINubEv",
        "outputId": "18bf9e9c-647c-4d8c-8750-d07a813b599f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['litre',\n",
              " 'literacy',\n",
              " 'listing',\n",
              " 'leaky',\n",
              " 'laundering',\n",
              " 'lashes',\n",
              " 'lace',\n",
              " 'koch',\n",
              " 'kane',\n",
              " 'juvenile']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer((text_data[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NII9DzNqvIKz",
        "outputId": "8bbe8fdc-fc28-4423-bfd2-19502b8619f5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(17,), dtype=int64, numpy=\n",
              "array([    6,   237, 11652,  2850,     6,    83,    93,  3151,    47,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[6], vocab[237], vocab[11652], vocab[2850], vocab[83], vocab[93]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9M2Wx7hvPQ3",
        "outputId": "62d1fbb9-1c01-4d74-f00c-4b1825675cf0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('i', 'am', 'soooo', 'glad', \"'m\", 'not')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Training Set"
      ],
      "metadata": {
        "id": "rl7r-K0Jvo8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_inputs(text):\n",
        "  text = tf.expand_dims(text, axis=-1)\n",
        "  tokenized_sentences = vectorize_layer(text)\n",
        "  x = tokenized_sentences[:,:-1]\n",
        "  y = tokenized_sentences[:,1:]\n",
        "\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "AZ-XAJX9vwQn"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = text_ds.map(prepare_inputs)"
      ],
      "metadata": {
        "id": "g7xB9rR0vxv3"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qd5xWsCvzt5",
        "outputId": "f5b00ea1-e390-435c-867c-54d8e3a358fe"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=(TensorSpec(shape=(None, 16), dtype=tf.int64, name=None), TensorSpec(shape=(None, 16), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(None, ), dtype=\"int32\")\n",
        "x = layers.Embedding(15000, 100)(inputs)\n",
        "x = layers.LSTM(128, return_sequences=True)(x)\n",
        "x = layers.LSTM(128, return_sequences=True)(x)\n",
        "\n",
        "outputs = layers.Dense(15000, activation=\"softmax\")(x)\n",
        "\n",
        "lstm = models.Model(inputs, outputs)\n",
        "lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcBdWES0v0gk",
        "outputId": "46d27e37-a758-4dd6-9a78-ebb0d5f79bf1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 100)         1500000   \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, None, 128)         117248    \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, None, 128)         131584    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, None, 15000)       1935000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3683832 (14.05 MB)\n",
            "Trainable params: 3683832 (14.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm.compile(loss= losses.SparseCategoricalCrossentropy(),\n",
        "             optimizer=\"adam\")"
      ],
      "metadata": {
        "id": "p7CYUW8uwC3f"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index for index, word in enumerate(index_to_word)\n",
        "        }  # <1>\n",
        "\n",
        "    def sample_from(self, probs, temperature):  # <2>\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]  # <3>\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:  # <4>\n",
        "            x = np.array([start_tokens])\n",
        "            y = self.model.predict(x, verbose=0)  # <5>\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)  # <6>\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            start_tokens.append(sample_token)  # <7>\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        wordx = np.random.randint(1, 10000)\n",
        "        vocabx= vocab[wordx]\n",
        "        self.generate(f\"{vocabx}\", max_tokens=100, temperature=1.0)"
      ],
      "metadata": {
        "id": "7czhnCaAwNr3"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath=\"./checkpoint/checkpoint.ckpt\",\n",
        "    save_weights_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "text_generator = TextGenerator(vocab)"
      ],
      "metadata": {
        "id": "kr6qp4pbwSgW"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm.fit(\n",
        "    train_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGJssiKpwUnW",
        "outputId": "b997b795-67c3-4989-eda0-83333450ba49"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "   5/7813 [..............................] - ETA: 5:16 - loss: 2.3568"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0258s vs `on_train_batch_end` time: 0.0536s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7811/7813 [============================>.] - ETA: 0s - loss: 2.2591\n",
            "generated text:\n",
            "questioning  that r /jokes writer is like popeye 's daughter . \n",
            "\n",
            "7813/7813 [==============================] - 175s 22ms/step - loss: 2.2590\n",
            "Epoch 2/10\n",
            "7812/7813 [============================>.] - ETA: 0s - loss: 2.1839\n",
            "generated text:\n",
            "commonly  people cause a common loan jokes ! joke . . . \n",
            "\n",
            "7813/7813 [==============================] - 174s 22ms/step - loss: 2.1839\n",
            "Epoch 3/10\n",
            "7811/7813 [============================>.] - ETA: 0s - loss: 2.1334\n",
            "generated text:\n",
            "scariest  man at a party \n",
            "\n",
            "7813/7813 [==============================] - 173s 22ms/step - loss: 2.1333\n",
            "Epoch 4/10\n",
            "7812/7813 [============================>.] - ETA: 0s - loss: 2.0970\n",
            "generated text:\n",
            "slice  cup from political humor goes into heaven . \n",
            "\n",
            "7813/7813 [==============================] - 173s 22ms/step - loss: 2.0970\n",
            "Epoch 5/10\n",
            "7813/7813 [==============================] - ETA: 0s - loss: 2.0692\n",
            "generated text:\n",
            "butcher  's dreams \n",
            "\n",
            "7813/7813 [==============================] - 171s 22ms/step - loss: 2.0692\n",
            "Epoch 6/10\n",
            "7812/7813 [============================>.] - ETA: 0s - loss: 2.0470\n",
            "generated text:\n",
            "reduce  : a woman makes both tomatoes \n",
            "\n",
            "7813/7813 [==============================] - 171s 22ms/step - loss: 2.0470\n",
            "Epoch 7/10\n",
            "7812/7813 [============================>.] - ETA: 0s - loss: 2.0288\n",
            "generated text:\n",
            "melons  have abortions \n",
            "\n",
            "7813/7813 [==============================] - 168s 22ms/step - loss: 2.0288\n",
            "Epoch 8/10\n",
            "7813/7813 [==============================] - ETA: 0s - loss: 2.0136\n",
            "generated text:\n",
            "kraft  still bought my girlfriend kill her siblings . . \n",
            "\n",
            "7813/7813 [==============================] - 170s 22ms/step - loss: 2.0136\n",
            "Epoch 9/10\n",
            "7813/7813 [==============================] - ETA: 0s - loss: 2.0007\n",
            "generated text:\n",
            "horseback  riding time 's date \n",
            "\n",
            "7813/7813 [==============================] - 172s 22ms/step - loss: 2.0007\n",
            "Epoch 10/10\n",
            "7811/7813 [============================>.] - ETA: 0s - loss: 1.9895\n",
            "generated text:\n",
            "lumber  matters . \n",
            "\n",
            "7813/7813 [==============================] - 171s 22ms/step - loss: 1.9895\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cfb842ed8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "        print(\"--------\\n\")"
      ],
      "metadata": {
        "id": "NC7y1IWCwuIX"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"black\", max_tokens=10, temperature=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By6OFRm4zCtL",
        "outputId": "7e5dddcf-52cf-4927-820e-3f7aac27a83f"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "black for disappointment \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"black\", max_tokens=10, temperature=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqKYM7bu_gOX",
        "outputId": "f7b8e22d-a0c2-40ee-ef04-ad53e0fe423e"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "black history jokes are like bed \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqB1ilIizFV-",
        "outputId": "87c9c760-aeaa-4bb4-de66-2c54c04d7158"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: black\n",
            "people:   \t19.06%\n",
            "lives:   \t7.51%\n",
            "guy:   \t6.23%\n",
            "jokes:   \t4.72%\n",
            "humor:   \t3.44%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: black history\n",
            ":   \t10.57%\n",
            "teacher:   \t9.71%\n",
            "joke:   \t7.72%\n",
            "is:   \t4.31%\n",
            "[UNK]:   \t4.21%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: black history jokes\n",
            ":   \t43.21%\n",
            ".:   \t11.73%\n",
            "are:   \t6.93%\n",
            "::   \t5.84%\n",
            "-:   \t4.64%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: black history jokes are\n",
            "like:   \t20.86%\n",
            "not:   \t6.7%\n",
            "the:   \t6.69%\n",
            "so:   \t3.96%\n",
            "great:   \t3.8%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: black history jokes are like\n",
            "food:   \t6.48%\n",
            "[UNK]:   \t5.09%\n",
            "women:   \t3.82%\n",
            "a:   \t3.4%\n",
            "the:   \t2.75%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: black history jokes are like bed\n",
            ":   \t18.55%\n",
            ".:   \t14.73%\n",
            "and:   \t10.1%\n",
            ",:   \t4.58%\n",
            "[UNK]:   \t3.15%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_text_from_model(model, index_to_word, start_prompt, max_tokens=100, temperature=1.0):\n",
        "    word_to_index = {word: index for index, word in enumerate(index_to_word)}\n",
        "\n",
        "    def sample_from(probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    start_tokens = [word_to_index.get(x, 1) for x in start_prompt.split()]\n",
        "    sample_token = None\n",
        "    generated_text = \"\"\n",
        "\n",
        "    while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "        x = np.array([start_tokens])\n",
        "        y = model.predict(x, verbose=0)\n",
        "        sample_token, _ = sample_from(y[0][-1], temperature)\n",
        "        start_tokens.append(sample_token)\n",
        "        generated_text += \" \" + index_to_word[sample_token]\n",
        "\n",
        "    return generated_text.strip()"
      ],
      "metadata": {
        "id": "I6pq6tN1zRnH"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm\n",
        "index_to_word = vocab"
      ],
      "metadata": {
        "id": "XkWr_vl_zb7E"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_prompt = \"i like  \"\n",
        "\n",
        "generated_text = generate_text_from_model(model, index_to_word, start_prompt, temperature=0.3)\n",
        "print(f\"Generated text:\\n{start_prompt + generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXG484-KzeQ0",
        "outputId": "6b6e1c56-e43f-4149-e33e-6afb5988b384"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "i like  my women like i like my coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordx = np.random.randint(1, 10000)\n",
        "vocabx= vocab[wordx]\n",
        "start_prompt = vocabx +\" \"\n",
        "\n",
        "generated_text = generate_text_from_model(model, index_to_word, start_prompt, temperature=1)\n",
        "print(f\"Generated text:\\n{start_prompt + generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsQRp3SVzgxg",
        "outputId": "5e21867a-1317-4423-f90b-82eec46253b0"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "tin did u hear a joke about food poisoning ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordx = np.random.randint(1, 10000)\n",
        "vocabx= vocab[wordx]\n",
        "start_prompt = vocabx +\" \"\n",
        "\n",
        "generated_text = generate_text_from_model(model, index_to_word, start_prompt, temperature=0.8)\n",
        "print(f\"Generated text:\\n{start_prompt + generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLZw4Wek1uTU",
        "outputId": "ec85e7a4-f11e-4e8c-ee2c-3815376d0060"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "nailed a good day , but i 'm supposed to love it in the band\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordx = np.random.randint(1, 10000)\n",
        "vocabx= vocab[wordx]\n",
        "start_prompt = vocabx +\" \"\n",
        "\n",
        "generated_text = generate_text_from_model(model, index_to_word, start_prompt, temperature=0.8)\n",
        "print(f\"Generated text:\\n{start_prompt + generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOTKQUyF2OC0",
        "outputId": "8c436feb-a787-4af5-8cd7-8b2e9f73a2bd"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "noticed me my wife made me . . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordx = np.random.randint(1, 10000)\n",
        "vocabx= vocab[wordx]\n",
        "start_prompt = vocabx +\" \"\n",
        "\n",
        "generated_text = generate_text_from_model(model, index_to_word, start_prompt, temperature=0.8)\n",
        "print(f\"Generated text:\\n{start_prompt + generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vds638qu2S_B",
        "outputId": "6f2cb350-137c-40d8-ee76-4ef35a22798f"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "historians call me a minecraft movie .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordx = np.random.randint(1, 15000)\n",
        "vocabx= vocab[wordx]\n",
        "start_prompt = vocabx +\" \"\n",
        "\n",
        "generated_text = generate_text_from_model(model, index_to_word, start_prompt, temperature=0.8)\n",
        "print(f\"Generated text:\\n{start_prompt + generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbwKbqqC2Y4d",
        "outputId": "27f5daf2-2a45-43d4-d1b5-ba8f14f5e049"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function AtomicFunction.__del__ at 0x7cfc5363f5b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 286, in __del__\n",
            "    def __del__(self):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "pistol interviewer : sir , can i get some money for a [UNK] that ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordx = np.random.randint(1, 15000)\n",
        "vocabx= vocab[wordx]\n",
        "start_prompt = vocabx +\" \"\n",
        "\n",
        "generated_text = generate_text_from_model(model, index_to_word, start_prompt, temperature=0.8)\n",
        "print(f\"Generated text:\\n{start_prompt + generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUt3BH7XAKk2",
        "outputId": "35d760b4-1477-424c-dd7e-85f76ccf8f99"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "forks are made in the car if they were taking a bath .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"my brother\", max_tokens=10, temperature=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nOOBgvEAnlH",
        "outputId": "16f41268-7a1e-447f-e28c-fae150bed088"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "my brother 's law degree walks into a bar \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"my brother\", max_tokens=10, temperature=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXiYKt8SBTrN",
        "outputId": "44a7a4bc-7d7a-4c4b-f67b-e25a4ffe8260"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "my brother said he wanted to be a comedian .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"why\", max_tokens=20, temperature=0.8\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtpyTHfADRQN",
        "outputId": "77817224-9585-4b5d-fe17-0e52fae87c14"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "why did the duck get arrested for a crime scene ? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"because\", max_tokens=10, temperature=0.8\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtUl9mnFDYWR",
        "outputId": "6ba9ad48-d4fe-4b15-e481-2225d7281251"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "because i named trump 'the kkk movie \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"because\", max_tokens=10, temperature=0.8\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDFGVMyaDgCY",
        "outputId": "9767bbb9-5329-41c5-9a36-f70b7bfa69f0"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "because trump 's what makes it do ? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"why\", max_tokens=10, temperature=0.8\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "219dmNAdCECl",
        "outputId": "8cf9cc01-ef69-42c7-dcfa-c0c97beb6460"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "why are the 'dark and black people so young ?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"because\", max_tokens=20, temperature=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7usDdCffCq_I",
        "outputId": "fdeb6d5f-5203-422d-accd-dcc9c0ee0f1c"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "because they just can be \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FJ8SaS_B7dX",
        "outputId": "83883866-af72-4f93-92ab-d746e5a9644b"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: because\n",
            "i:   \t50.73%\n",
            "of:   \t10.5%\n",
            "you:   \t10.2%\n",
            "it:   \t8.3%\n",
            "they:   \t2.7%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: because trump\n",
            "is:   \t32.95%\n",
            "'s:   \t20.93%\n",
            "and:   \t6.48%\n",
            ",:   \t4.93%\n",
            "has:   \t4.73%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: because trump 's\n",
            "[UNK]:   \t13.41%\n",
            "the:   \t6.36%\n",
            "immigration:   \t3.75%\n",
            "a:   \t3.17%\n",
            "presidency:   \t2.69%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: because trump 's what\n",
            "he:   \t23.96%\n",
            "'s:   \t8.49%\n",
            "is:   \t8.12%\n",
            "i:   \t5.78%\n",
            "the:   \t5.54%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: because trump 's what makes\n",
            "a:   \t28.81%\n",
            "him:   \t10.34%\n",
            "the:   \t9.06%\n",
            "[UNK]:   \t6.29%\n",
            "me:   \t6.17%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: because trump 's what makes it\n",
            "great:   \t9.78%\n",
            "a:   \t8.02%\n",
            "good:   \t7.72%\n",
            "[UNK]:   \t6.36%\n",
            "sad:   \t6.14%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: because trump 's what makes it do\n",
            "?:   \t12.71%\n",
            ".:   \t10.19%\n",
            ":   \t9.01%\n",
            "[UNK]:   \t6.2%\n",
            "the:   \t5.35%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: because trump 's what makes it do ?\n",
            ":   \t99.28%\n",
            "?:   \t0.29%\n",
            "[UNK]:   \t0.06%\n",
            "!:   \t0.06%\n",
            ".:   \t0.05%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chzgDauCCBP0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}